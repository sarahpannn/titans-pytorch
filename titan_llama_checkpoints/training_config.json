{
  "model_name": "titan-llama-1b",
  "vocab_size": 32000,
  "hidden_size": 2048,
  "intermediate_size": 5504,
  "num_hidden_layers": 22,
  "num_attention_heads": 32,
  "num_key_value_heads": 32,
  "max_position_embeddings": 2048,
  "segment_len": 512,
  "num_persist_mem_tokens": 4,
  "num_longterm_mem_tokens": 4,
  "neural_memory_layers": [
    4,
    8,
    12,
    16,
    20
  ],
  "neural_memory_segment_len": 64,
  "neural_memory_batch_size": 128,
  "neural_memory_depth": 2,
  "use_flex_attn": true,
  "sliding_window_attn": true,
  "neural_mem_gate_attn_output": false,
  "neural_mem_weight_residual": true,
  "total_tokens": 1000000,
  "batch_size": 4,
  "micro_batch_size": 1,
  "sequence_length": 2048,
  "gradient_accumulation_steps": 4,
  "learning_rate": 0.0003,
  "min_learning_rate": 3e-05,
  "weight_decay": 0.1,
  "beta1": 0.9,
  "beta2": 0.95,
  "grad_clip": 1.0,
  "neural_mem_learning_rate": 0.01,
  "neural_mem_momentum": 0.9,
  "neural_mem_weight_decay": 0.01,
  "warmup_steps": 2000,
  "eval_interval": 100,
  "save_interval": 500,
  "log_interval": 10,
  "dataset_name": "cerebras/SlimPajama-627B",
  "tokenizer_name": "meta-llama/Llama-2-7b-hf",
  "num_proc": 8,
  "use_ddp": false,
  "local_rank": -1,
  "world_size": 1,
  "output_dir": "./titan_llama_checkpoints",
  "wandb_project": "titan-llama-training",
  "wandb_run_name": "titan-llama-1b-1764522846",
  "log_level": "INFO",
  "resume_from_checkpoint": null
}